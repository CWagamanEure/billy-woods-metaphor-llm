{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate optuna scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'filepath'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, GPT2Tokenizer,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=1)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='right')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    mse = mse_metric.compute(predictions=predictions, references=labels)['mse']\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {\"rmse\": rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = os.getcwd()\n",
    "d = pd.read_csv(file_path)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "d['label'] = d['rating']\n",
    "d['inputs'] = \"A novel metaphor for \" + d['prompt'] + \" is: \" + d['response']\n",
    "d['text'] = d['inputs']\n",
    "d_input = d[['text', 'label', 'set']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(d_input)\n",
    "\n",
    "dataset_split = DatasetDict({\n",
    "    'train': dataset.filter(lambda example: example['set'] == 'training'),\n",
    "    'validation': dataset.filter(lambda example: example['set'] == 'validation'),\n",
    "    'test': dataset.filter(lambda example: example['set'] == 'test'),\n",
    "    'heldout': dataset.filter(lambda example: example['set'] == 'heldout')\n",
    "}).remove_columns('set')\n",
    "\n",
    "tokenized_datasets = dataset_split.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2.47e-5,\n",
    "    num_train_epochs=139,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=100,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    #compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = trainer.predict(tokenized_datasets['test'])\n",
    "test_df = pd.DataFrame({\n",
    "    'text': tokenized_datasets['test']['text'],\n",
    "    'label': tokenized_datasets['test']['label'],\n",
    "    'prediction': np.squeeze(prediction.predictions)\n",
    "})\n",
    "test_df.to_csv(\"/content/test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "df_test = pd.read_csv(\"test_predictions.csv\")\n",
    "df_test.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df_test['label'], df_test['prediction'], alpha=0.6)\n",
    "plt.plot([-2, 2], [-2, 2], 'r--')\n",
    "plt.xlabel(\"Human Label (z-score)\")\n",
    "plt.ylabel(\"Model Prediction\")\n",
    "plt.title(\"Predicted vs. Human Creativity Ratings\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "r, _ = pearsonr(df_test['label'], df_test['prediction'])\n",
    "print(f\"Pearson correlation: {r:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = df_test['label'] - df_test['prediction']\n",
    "plt.hist(residuals, bins=30, edgecolor='black')\n",
    "plt.title(\"Distribution of Prediction Errors (Residuals)\")\n",
    "plt.xlabel(\"Error (Label - Prediction)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"name\", token = \"token\")\n",
    "tokenizer.push_to_hub(\"name\", token = \"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "model_path = \"modelname\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/content/base_mistral_metaphors.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs = tokenizer(list(df['text']), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "df['predicted_creativity'] = predictions\n",
    "\n",
    "\n",
    "df.to_csv(\"scored_metaphors.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_creativity'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = df.sort_values(by=\"predicted_creativity\", ascending=False).head(10)\n",
    "bottom = df.sort_values(by=\"predicted_creativity\", ascending=True).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.to_csv(\"top_metaphors.csv\", index=False)\n",
    "bottom.to_csv(\"bottom_metaphors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['predicted_creativity'], bins=20, edgecolor='black')\n",
    "plt.title(\"Distribution of Predicted Creativity Scores\")\n",
    "plt.xlabel(\"Creativity Score\")\n",
    "plt.ylabel(\"Number of Metaphors\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'] = df['text'].str.extract(r'for a (.+?) is', expand=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"topic\")[\"predicted_creativity\"].mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"topic\")[\"predicted_creativity\"].mean().sort_values().plot(kind=\"barh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
