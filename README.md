# ğŸ¤ Metaphor Generation with Billy Woodsâ€“Fine-Tuned LLM

This project explores whether large language models can generate or evaluate **metaphorical creativity**, and whether fine-tuning them on a **stylistically rich corpus** â€” in this case, the abstract, politically-charged lyrics of hip-hop artist Billy Woods â€” improves their performance.

> â€œA thousand plateaus, a constellation of prisons / An ocean of archipelagos, an algorithmâ€¦â€  
> â€” *Billy Woods, â€œWishing Badâ€*

---

## ğŸ§  Project Summary

- Fine-tuned a **Mistral 7B** language model on the complete lyrical archive of Billy Woods.
- Prompted the model with metaphor stems like:
  - `A police badge is like...`
  - `A bullet is like...`
  - `A jail cell is like...`
- Compared completions from the fine-tuned model with a **base model**.
- Evaluated metaphor creativity using a **GPT-2â€“based scoring model**, trained on human-rated metaphor data.
- Reflected on authorship, voice simulation, and ethical implications of using AI to mimic human artistic expression.

---

## ğŸ”— Model Access

| Model | Hugging Face Link |
|-------|-------------------|
| **Fine-tuned Mistral 7B** on Billy Woods lyrics | https://huggingface.co/cwagaman/billy_woods-mistral7b  |
| **GPT-2 Creativity Scorer** (https://huggingface.co/cwagaman/metaphor_scorer-GPT2) |

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

## ğŸ§  Metaphor Scoring Model

The creativity scorer used in this project is based on a methodology from:

**Di Stefano, Peter, et al.**  
_"Automatic Scoring of Metaphor Creativity with Large Language Models"
[Paper link](https://www.tandfonline.com/doi/full/10.1080/10400419.2024.2326343)

We re-implemented and fine-tuned a GPT-2 model using their dataset of metaphors rated by human annotators (-1 to +2).  
Our implementation achieved a Pearson correlation of 0.68 on the original test set.


billy-woods-metaphor-llm/
â”‚
â”œâ”€â”€ README.md                  â† You are here
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ final_script.md           â† Full symposium presentation
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ evaluate_finetuned_model.ipynb  â† Load model & score metaphor completions
â”‚   â””â”€â”€ metaphor_scorer_training.ipynb  â† (Optional) Fine-tune GPT-2 scorer
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ metaphor_outputs.csv   â† Base vs. fine-tuned model outputs
â”‚   â””â”€â”€ creativity_scores.csv  â† Scores assigned by the GPT-2 scorer
